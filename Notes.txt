Paper:

MTTS-CAN is an improvement to DeepPhys. Network completely based on 2D CNNs therfore takes only 6ms per frame. 
Potentially could be used for real-time HR measurement.

Experimental Setup:

During data acquisition, a multi-imager semicircular array (a total of 9 synchronized, visible spectrum imagers)
centered on the imaged participant in a controlled light environment was used to record the participantâ€™s head 
motions during specific tasks.

Each participant was recorded six times with increasing head motion in each task.

The six recordings were repeated twice in front of two backgrounds.



Questions:

In data_generator.py:

What is frame depth?    
Ans. Frame depth is the window size (number of adjacent frames)



Code (Given):

When running predict_vitals.py, the x-axis of graph obtained contains the number of frames. y-axis is wave amplitude
I think.

By looking at the code, I found out that the authors did preprocessing on the dataset outside the current code base. This included adding the 
video frames, labels (ground truth for each frame) and the frame normalization in a single matlab file for each video.

The files are then read using the h5py library.



Code (My Version):

Will work with only UBFC for now and if there's time, I'll use PURE.

Stored data (video frames and corresponding data in a hdf5 file for each subject)

Each frame has a corresponding label (ppg signal value). The last frame of the video was excluded as it did not have a 
succeeding frame for normalization.

The frames were resized to 36x36 pixels.

TODO:

Modify and test functions in pre_process.py

Go through all TODOs in the codebase.

Leave split_subj for now, will just use 3 subs for train and 2 for test (since atm I have 5 subjects total).

Check out how data generator works in tensorflow. Do I need to store the frames of all videos in the same np array as was done in 
data_generator.py ?








