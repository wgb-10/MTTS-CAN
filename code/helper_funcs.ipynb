{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to test helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import csv\n",
    "from glob import iglob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '20', '22', '23', '24', '25', '26', '27', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '8', '9', 'UBFC']\n",
      "['0', 'subject1', '_subject1_cropped', '_subject1_cropped_128x128', '_subject1_cropped_32x32']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['0', 'subject10', '_subject10_cropped', '_subject10_cropped_128x128', '_subject10_cropped_32x32']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['0', 'subject11', '_subject11_cropped', '_subject11_cropped_128x128', '_subject11_cropped_32x32']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['0', 'subject12']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject13']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject14']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject15']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject16']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject17']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject18']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject20']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject22']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject23']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject24']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject25']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject26']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject27']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject3']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject30']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject31']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject32']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject33']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject34']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject35']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject36']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject37']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject38']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject39']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject4']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject40']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject41']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject42']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject43']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject44']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject45']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject46']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject47']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject48']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject49']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject5']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject8']\n",
      "[]\n",
      "[]\n",
      "['0', 'subject9']\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for path, subdirs, files in os.walk(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\UBFC2\\DATASET_2'):\n",
    "    # for name in files:\n",
    "    #     print (os.path.join(path, name))\n",
    "    print(subdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder with min images: D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\UBFC2\\DATASET_2\\37\\subject37 \n",
      "file count:1170 \n",
      "\n",
      "folder with min size: D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\UBFC2\\DATASET_2\\37\\subject37 \n",
      " total size:417436621 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1170,\n",
       " WindowsPath('D:/OneDrive/Documents/rPPG-Projects/Datasets-Preprocessed/UBFC2/DATASET_2/37/subject37'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://codereview.stackexchange.com/questions/178818/checks-size-of-and-number-of-files-in-each-subdirectory\n",
    "# Accessed: 30/03/2022\n",
    "\n",
    "# Helper functions to find the minimum no. of files in the sub directories of a parent directory\n",
    "def folders_in_path(path):\n",
    "    if not Path.is_dir(path):\n",
    "        raise ValueError(\"argument is not a directory\")\n",
    "    yield from filter(Path.is_dir, path.iterdir())\n",
    "\n",
    "def folders_in_depth(path, depth):\n",
    "    if 0 > depth:\n",
    "        raise ValueError(\"depth smaller 0\")\n",
    "    if 0 == depth:\n",
    "        yield from folders_in_path(path)\n",
    "    else:\n",
    "        for folder in folders_in_path(path):\n",
    "            yield from folders_in_depth(folder, depth-1)\n",
    "\n",
    "def files_in_path(path):\n",
    "    if not Path.is_dir(path):\n",
    "        raise ValueError(\"argument is not a directory\")\n",
    "    yield from filter(Path.is_file, path.iterdir())\n",
    "\n",
    "def sum_file_size(filepaths):\n",
    "    return sum([filep.stat().st_size for filep in filepaths])\n",
    "\n",
    "\n",
    "# Function that returns the minimum number of frames of all videos in the dataset\n",
    "def get_min_num_of_frames(rootDir):\n",
    "\n",
    "    # Variables that will store the min no. of frames and the corresponding directory of the subject \n",
    "    min_n_files = float('inf')\n",
    "    min_n_files_folder = ''\n",
    "\n",
    "    for folder in folders_in_depth(rootDir,1):\n",
    "        \n",
    "        # If the last character in the folder name is 0, skip it (contains gt and not image frames)\n",
    "        if str(folder)[-1] == '0':\n",
    "            continue\n",
    "\n",
    "        n_files = len(list(files_in_path(folder)))\n",
    "        \n",
    "        if n_files < min_n_files:\n",
    "            min_n_files = n_files\n",
    "            min_n_files_folder = folder\n",
    "\n",
    "    print(f'folder with min images: {min_n_files_folder} \\nfile count:{min_n_files} \\n')\n",
    "\n",
    "    return min_n_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://realpython.com/storing-images-in-python/#reading-many-images (Accessed 18/03/2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_many_disk(num_images, imagesPath, gtPath):\n",
    "    images, labels = [], []\n",
    "\n",
    "\n",
    "     # For each frame\n",
    "    for imagePath in imagesPath:\n",
    "        \n",
    "        # Stop when the total images read = num_images (to keep the no. of images same across all subjects)\n",
    "        if len(images) == num_images:\n",
    "            break\n",
    "\n",
    "        # Store each frame \n",
    "        # print(f'[INFO] Working on Image: {image}')\n",
    "\n",
    "        # Read and resize the image\n",
    "        # Reference: https://pillow.readthedocs.io/en/stable/reference/Image.html (Accessed 21/03/2022)\n",
    "\n",
    "        with Image.open(imagePath) as image:\n",
    "            image_resized = image.resize((36, 36))\n",
    "            images.append(np.array(image_resized))\n",
    "        \n",
    "        # print(f'[INFO] images list contains: {len(images)} elements  of type {type(images[0])}')\n",
    "\n",
    "    with open(gtPath, \"r\") as csvfile:\n",
    "        reader = csv.reader(\n",
    "            csvfile, delimiter=\",\"\n",
    "        )\n",
    "\n",
    "        for idx, row in enumerate(reader):\n",
    "            \n",
    "            # Skip the title row\n",
    "            if idx > 0:\n",
    "                \n",
    "                # Skip the ppg recording for the last frame as it doesn't have a successor for normalization. \n",
    "                # This frame will only be used to normalize the 2nd last frame.\n",
    "                if len(labels) < num_images - 1:        \n",
    "                    ppg = float(row[2])                 # row[2] is the column containing ppg signal (label)\n",
    "                    # print(f'[INFO] ppg: {ppg}')\n",
    "                    labels.append(ppg)  \n",
    "\n",
    "    # print(f'[INFO] labels list contains: {len(labels)} elements  of type {type(labels[0])}')\n",
    "\n",
    "    # List containing the images with normailzed frames added in the 3rd dimension\n",
    "    expanded_images = []\n",
    "\n",
    "    # Perform frame normalization using every two adjacent frames as (c(t + 1) - c(t))/(c(t) + c(t + 1))\n",
    "    # where c is the channel of the frame.\n",
    "    for idx, image in enumerate(images):\n",
    "        if idx < len(images) - 1:\n",
    "            for i in range(3):\n",
    "\n",
    "                # print(f'[INFO] Shape of Frame {idx}: {(images[idx][:, :, i]).shape}')\n",
    "\n",
    "                # Displaying the frame at channel i\n",
    "                # display_frame(images[idx][:, :, i], f'Frame {idx} Channel {i}')   \n",
    "\n",
    "                # Normalized frame calculated by the formula above\n",
    "                normalizedFrame = (images[idx + 1][:, :, i] - images[idx][:, :, i]\n",
    "                ) / (images[idx][:, :, i] + images[idx + 1][:, :, i])\n",
    "\n",
    "                # Displaying the normalized frame at channel i\n",
    "                # display_frame(normalizedFrame, f'Normalized Frame {idx} Channel {i}')\n",
    "\n",
    "                # print(f'[INFO] Shape of Normalized Frame {idx}: {normalizedFrame.shape}')\n",
    "\n",
    "                # Adding an extra dimension to the normalized frame to make it possible to append to original image\n",
    "                normalizedFrame = np.expand_dims(normalizedFrame, axis=2)\n",
    "\n",
    "                image = np.append(image, normalizedFrame, axis=2)\n",
    "            \n",
    "            #     print(f'shape of normalizedFrame: {normalizedFrame.shape}')\n",
    "            #     print(f'shape of image: {image.shape}')\n",
    "\n",
    "            # print(f'shape of image after going through each channel: {image.shape}')\n",
    "            \n",
    "            # Storing the expanded images \n",
    "            expanded_images.append(image)\n",
    "            # break\n",
    "\n",
    "\n",
    "    return np.array(expanded_images), np.array(labels)\n",
    "\n",
    "    # # Loop over all IDs and read each image in one by one\n",
    "    # for image_id in range(num_images):\n",
    "    #     images.append(np.array(Image.open(disk_dir / f\"{image_id}.png\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_hdf5(target_dir, subID, images, labels):\n",
    "    \"\"\" Stores an array of images to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        target_dir:  path to the directory where the HDF5 file will be stored.\n",
    "        subID:       subject ID.\n",
    "        images       images array, (N, W, H, NC) to be stored (where N: number of images, W: width, H: height, NC: number of channels).\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pathToTarget    path to the HDF5 file.\n",
    "    \"\"\"\n",
    "\n",
    "    pathToTarget = os.path.join(target_dir, f\"{subID}.h5\")\n",
    "\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(pathToTarget, \"w\")\n",
    "\n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"images\", data=images\n",
    "    )\n",
    "    meta_set = file.create_dataset(\n",
    "        \"labels\", data=labels\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    return pathToTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_many_hdf5(file_path):\n",
    "    \"\"\" Reads image from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        path   path to file\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images       images array, (N, W, H, NC) to be stored (where N: number of images, W: width, H: height, NC: number of channels).\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(file_path, \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/images\"]).astype(\"float64\")\n",
    "    labels = np.array(file[\"/labels\"]).astype(\"float64\")\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subj(data_dir, cv_split):\n",
    "    \"\"\" Splits the data from data_dir into train and test sets.\n",
    "    Parameters:\n",
    "    ---------------\n",
    "    data_dir:  path to the directory containing the data.\n",
    "    cv_split:  percentage of the data to be used for testing (written as a float, e.g. 50% = 0.5).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    subTrain: list of paths to the training data.\n",
    "    subTest:  list of paths to the testing data.\n",
    "\"\"\"\n",
    "    # Get the total no. of subjects\n",
    "    num_sub = len(os.listdir(data_dir))\n",
    "\n",
    "    # Store the paths of each subject into a list\n",
    "    sub_paths = [os.path.join(data_dir, sub) for sub in os.listdir(data_dir)]\n",
    "\n",
    "    # Get the no. of training paths\n",
    "    num_train = int(num_sub * cv_split)\n",
    "    \n",
    "    # Create a list of training paths\n",
    "    subTrain = sub_paths[:num_train]\n",
    "\n",
    "    # Create a list of testing paths\n",
    "    subTest = sub_paths[num_train:]\n",
    "\n",
    "    return subTrain, subTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject1.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject10.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject11.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject12.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject13.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject14.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject15.h5'],\n",
       " ['D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject16.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject17.h5',\n",
       "  'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\hdf5\\\\UBFC2\\\\DATASET_2\\\\subject18.h5'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_subj(Path(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\hdf5\\UBFC2\\DATASET_2'), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wesle\\AppData\\Local\\Temp\\ipykernel_980\\201566283.py:54: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalizedFrame = (images[idx + 1][:, :, i] - images[idx][:, :, i]\n",
      "C:\\Users\\wesle\\AppData\\Local\\Temp\\ipykernel_980\\201566283.py:54: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  normalizedFrame = (images[idx + 1][:, :, i] - images[idx][:, :, i]\n"
     ]
    }
   ],
   "source": [
    "# Get the number of frames\n",
    "nFrames = get_min_num_of_frames(Path(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\UBFC2\\DATASET_2'))\n",
    "\n",
    "# Get iterator over different subjects\n",
    "imageDirs = iglob(\"D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\UBFC2\\\\DATASET_2\\\\[0-9]*\\\\subject[0-9]*\")\n",
    "\n",
    "# Lists that will contain the images, labels and the subject IDs\n",
    "images, labels, subjects = [], [], []\n",
    "\n",
    "# For each subject\n",
    "for idx, path_ in enumerate(imageDirs):\n",
    "    # print(f\"[INFO] Working in {path_}\")\n",
    "\n",
    "    if idx > 9:\n",
    "        break  # Testing only for first 10 subjects (for now)\n",
    "  \n",
    "    # print(f'[INFO] Number of images: {num_images}')\n",
    "    \n",
    "    # Get the path to the frames\n",
    "    imagesPath = iglob(os.path.join(path_, \"*.png\"))\n",
    "\n",
    "    # Get subject number from path\n",
    "    subID = path_.split(\"\\\\\")[-1]\n",
    "\n",
    "    # Add the subject ID to subjects list\n",
    "    subjects.append(subID)\n",
    "\n",
    "    # Get the path to the csv file\n",
    "    gtPath = path_.replace(subID, r\"0\\phys.csv\")    \n",
    "\n",
    "    images, labels = read_many_disk(nFrames, imagesPath, gtPath) \n",
    "\n",
    "     # Create a new directory for the hdf5 file of subject\n",
    "    hdf5_dir = Path(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\hdf5\\UBFC2\\DATASET_2')\n",
    "    if not os.path.exists(hdf5_dir): \n",
    "        hdf5_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Store the images and labels at the target path\n",
    "    storedFilePath = store_many_hdf5(hdf5_dir, subID, images, labels)\n",
    "\n",
    "    # Reading the stored data for each subject\n",
    "    images_, labels_ = read_many_hdf5(storedFilePath)\n",
    "\n",
    "    # Checking if the read_image function works by comparing stored images and labels with the images and labels \n",
    "    # read from the file \n",
    "    assert(images.all()==images_.all())\n",
    "    assert(labels.all()==labels_.all())\n",
    "\n",
    "# print(f'shape of images: {np.shape(images)}, type: {type(images)}')\n",
    "# print(f'shape of labels: {np.sha  pe(labels)}, type: {type(labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "721881c4ebbfc4a48f73e756600484e3f3653201792f48d5e9f9bb135cb791ab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
