{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to test helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import csv\n",
    "from glob import iglob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://realpython.com/storing-images-in-python/#reading-many-images (Accessed 18/03/2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Image.open(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\UBFC2\\DATASET_2\\1\\subject1\\0.png') as image:\n",
    "#     print(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Image.open(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\UBFC2\\DATASET_2\\1\\subject1\\0.png') as image:\n",
    "#     image = np.array(image) /255\n",
    "#     print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_many_disk(num_images_required, imagesPath, gtPath):\n",
    "    \n",
    "    images, labels = [], []\n",
    "\n",
    "    # Variable to count the number of images in the folder\n",
    "    num_images = 0\n",
    "\n",
    "     # For each frame\n",
    "    for imagePath in imagesPath:\n",
    "        \n",
    "        # Store each frame \n",
    "        # print(f'[INFO] Working on Image: {image}')\n",
    "\n",
    "        # Read and resize the image\n",
    "        # Reference: https://pillow.readthedocs.io/en/stable/reference/Image.html (Accessed 21/03/2022)\n",
    "\n",
    "        with Image.open(imagePath) as image:\n",
    "            image_resized = image.resize((36, 36))\n",
    "\n",
    "            # Normalize the image\n",
    "            image_normalized = np.array(image_resized) / 255.0\n",
    "\n",
    "            images.append(image_normalized)\n",
    "        \n",
    "        num_images += 1\n",
    "\n",
    "        # print(f'[INFO] images list contains: {len(images)} elements  of type {type(images[0])}')\n",
    "\n",
    "\n",
    "    with open(gtPath, \"r\") as csvfile:\n",
    "        reader = csv.reader(\n",
    "            csvfile, delimiter=\",\"\n",
    "        )\n",
    "\n",
    "        for idx, row in enumerate(reader):\n",
    "            \n",
    "            # Skip the title row\n",
    "            if idx > 0:\n",
    "                \n",
    "                # Skip the ppg recording for the last frame as it doesn't have a successor for normalization. \n",
    "                # This frame will only be used to normalize the 2nd last frame.\n",
    "                if len(labels) < num_images - 1:        \n",
    "                    ppg = float(row[2])                 # row[2] is the column containing ppg signal (label)\n",
    "                    # print(f'[INFO] ppg: {ppg}')\n",
    "                    labels.append(ppg)  \n",
    "\n",
    "    # print(f'[INFO] labels list contains: {len(labels)} elements  of type {type(labels[0])}')\n",
    "\n",
    "    # List containing the images with normailzed frames added in the 3rd dimension\n",
    "    expanded_images = []\n",
    "\n",
    "    # Perform frame normalization using every two adjacent frames as (c(t + 1) - c(t))/(c(t) + c(t + 1))\n",
    "    # where c is the channel of the frame.\n",
    "    for idx, image in enumerate(images):\n",
    "        if idx < num_images - 1:\n",
    "            for i in range(3):\n",
    "\n",
    "                # print(f'[INFO] Shape of Frame {idx}: {(images[idx][:, :, i]).shape}')\n",
    "\n",
    "                # Displaying the frame at channel i\n",
    "                # display_frame(images[idx][:, :, i], f'Frame {idx} Channel {i}')   \n",
    "\n",
    "                # Normalized frame calculated by the formula above\n",
    "                normalizedFrame = (images[idx + 1][:, :, i] - images[idx][:, :, i]\n",
    "                ) / (images[idx][:, :, i] + images[idx + 1][:, :, i])\n",
    "\n",
    "                # Displaying the normalized frame at channel i\n",
    "                # display_frame(normalizedFrame, f'Normalized Frame {idx} Channel {i}')\n",
    "\n",
    "                # print(f'[INFO] Shape of Normalized Frame {idx}: {normalizedFrame.shape}')\n",
    "\n",
    "                # Adding an extra dimension to the normalized frame to make it possible to append to original image\n",
    "                normalizedFrame = np.expand_dims(normalizedFrame, axis=2)\n",
    "\n",
    "                image = np.append(image, normalizedFrame, axis=2)\n",
    "            \n",
    "            #     print(f'shape of normalizedFrame: {normalizedFrame.shape}')\n",
    "            #     print(f'shape of image: {image.shape}')\n",
    "\n",
    "            # print(f'shape of image after going through each channel: {image.shape}')\n",
    "            \n",
    "            # Storing the expanded images \n",
    "            expanded_images.append(image)\n",
    "\n",
    "    # print(f'Adding extra blank images and labels')\n",
    "\n",
    "    # Add blank images to the expanded images list until the length of the list is equal to the number of images required\n",
    "    while len(expanded_images) < num_images_required:\n",
    "        expanded_images.append(np.zeros((36, 36, 6)))\n",
    "\n",
    "    # print(f'Done adding blank images')\n",
    "\n",
    "    # Add blank labels to the labels list until the length of the list is equal to the number of images required\n",
    "    while len(labels) < num_images_required:\n",
    "        labels.append(0.0)\n",
    "\n",
    "    # For each image in the expanded images, subtract the mean and scale to unit standard deviation\n",
    "    for idx, image in enumerate(expanded_images):\n",
    "        for i in range(6):\n",
    "            image[:, :, i] -= np.mean(image[:, :, i])\n",
    "            image[:, :, i] /= np.std(image[:, :, i])\n",
    "        expanded_images[idx] = image\n",
    "\n",
    "\n",
    "    # print(f'Done adding blank labels, returning from function')\n",
    "\n",
    "    return np.array(expanded_images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_hdf5(target_dir, subID, images, labels):\n",
    "    \"\"\" Stores an array of images to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        target_dir:  path to the directory where the HDF5 file will be stored.\n",
    "        subID:       subject ID.\n",
    "        images       images array, (N, W, H, NC) to be stored (where N: number of images, W: width, H: height, NC: number of channels).\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pathToTarget    path to the HDF5 file.\n",
    "    \"\"\"\n",
    "\n",
    "    pathToTarget = os.path.join(target_dir, f\"{subID}.h5\")\n",
    "\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(pathToTarget, \"w\")\n",
    "\n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"images\", data=images\n",
    "    )\n",
    "    meta_set = file.create_dataset(\n",
    "        \"labels\", data=labels\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    return pathToTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5(file_path):\n",
    "    \"\"\" Reads images from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        path   path to file\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images       images array, (N, W, H, NC) to be stored (where N: number of images, W: width, H: height, NC: number of channels).\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(file_path, \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/images\"]).astype(\"float64\")\n",
    "    labels = np.array(file[\"/labels\"]).astype(\"float64\")\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = read_hdf5(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\hdf5\\UBFC2\\DATASET_2\\subject1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subj(data_dir, cv_split):\n",
    "    \"\"\" Splits the data from data_dir into train and test sets.\n",
    "    Parameters:\n",
    "    ---------------\n",
    "    data_dir:  path to the directory containing the data.\n",
    "    cv_split:  percentage of the data to be used for testing (written as a float, e.g. 50% = 0.5).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    subTrain: list of paths to the training data.\n",
    "    subTest:  list of paths to the testing data.\n",
    "\"\"\"\n",
    "    # Get the total no. of subjects\n",
    "    num_sub = len(os.listdir(data_dir))\n",
    "\n",
    "    # Store the paths of each subject into a list\n",
    "    sub_paths = [os.path.join(data_dir, sub) for sub in os.listdir(data_dir)]\n",
    "\n",
    "    # Get the no. of training paths\n",
    "    num_train = int(num_sub * cv_split)\n",
    "    \n",
    "    # Create a list of training paths\n",
    "    subTrain = sub_paths[:num_train]\n",
    "\n",
    "    # Create a list of testing paths\n",
    "    subTest = sub_paths[num_train:]\n",
    "\n",
    "    return subTrain, subTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function that returns the minimum number of frames of all videos in the dataset\n",
    "# def get_min_num_of_frames(globExp):\n",
    "\n",
    "#     # Variables that will store the min no. of frames and the corresponding directory of the subject \n",
    "#     min_num_files = float('inf')\n",
    "#     folder_with_min_num_files = ''\n",
    "\n",
    "#     # Get iterator over different subjects\n",
    "#     imageDirs = iglob(globExp)\n",
    "\n",
    "#     for path_ in imageDirs:\n",
    "\n",
    "#         num_images = len(os.listdir(path_))\n",
    "        \n",
    "#         if num_images < min_num_files:\n",
    "#             min_num_files = num_images\n",
    "#             folder_with_min_num_files = path_\n",
    "\n",
    "#     # print(f'folder with min images: { folder_with_min_num_files} \\nfile count:{min_num_files} \\n')\n",
    "\n",
    "#     return min_num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the maximum number of frames of all videos in the dataset\n",
    "def get_max_num_of_frames(globExp):\n",
    "\n",
    "    # Variables that will store the max no. of frames and the corresponding directory of the subject \n",
    "    max_num_files = -1\n",
    "    folder_with_max_num_files = ''\n",
    "\n",
    "    # Get iterator over different subjects\n",
    "    imageDirs = iglob(globExp)\n",
    "\n",
    "    for path_ in imageDirs:\n",
    "\n",
    "        num_images = len(os.listdir(path_))\n",
    "        \n",
    "        if num_images > max_num_files:\n",
    "            max_num_files = num_images\n",
    "            folder_with_max_num_files = path_\n",
    "\n",
    "    # print(f'folder with max images: { folder_with_max_num_files} \\nfile count:{max_num_files} \\n')\n",
    "\n",
    "    return max_num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_max_num_of_frames(r'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\UBFC2\\\\DATASET_2\\\\[0-9]*\\\\subject[0-9]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get iterator over different subjects\n",
    "imageDirs = iglob(\"D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\UBFC2\\\\DATASET_2\\\\[0-9]*\\\\subject[0-9]*\")\n",
    "\n",
    "# Lists that will contain the images, labels and the subject IDs\n",
    "images, labels, subjects = [], [], []\n",
    "\n",
    "# Get the maximum number of frames\n",
    "max_num_images = get_max_num_of_frames(r'D:\\\\OneDrive\\\\Documents\\\\rPPG-Projects\\\\Datasets-Preprocessed\\\\UBFC2\\\\DATASET_2\\\\[0-9]*\\\\subject[0-9]*')\n",
    "\n",
    "# Make maximum no. of images a multiple of 10 (add extra frames)\n",
    "max_num_images = max_num_images + (10 - max_num_images % 10)\n",
    "\n",
    "# For each subject\n",
    "for idx, path_ in enumerate(imageDirs):\n",
    "    # print(f\"[INFO] Working in {path_}\")\n",
    "\n",
    "    # if idx > 9:\n",
    "    #     break  # Testing only for first 10 subjects (for now)\n",
    "    \n",
    "    # print(f'[INFO] New maximum number of images: {max_num_images}')\n",
    "    \n",
    "    # Get the path to the frames\n",
    "    imagesPath = iglob(os.path.join(path_, \"*.png\"))\n",
    "\n",
    "    # Get subject number from path\n",
    "    subID = path_.split(\"\\\\\")[-1]\n",
    "\n",
    "    # Add the subject ID to subjects list\n",
    "    subjects.append(subID)\n",
    "\n",
    "    # Get the path to the csv file\n",
    "    gtPath = path_.replace(subID, r\"0\\phys.csv\")    \n",
    "\n",
    "    images, labels = read_many_disk(max_num_images, imagesPath, gtPath)\n",
    "\n",
    "     # Create a new directory for the hdf5 file of subject\n",
    "    hdf5_dir = Path(r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\hdf5\\UBFC2\\DATASET_2')\n",
    "    if not os.path.exists(hdf5_dir): \n",
    "        hdf5_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Store the images and labels at the target path\n",
    "    storedFilePath = store_many_hdf5(hdf5_dir, subID, images, labels)\n",
    "\n",
    "    # print(f'[INFO] Stored file at {storedFilePath}')\n",
    "\n",
    "    # Reading the stored data for each subject\n",
    "    images_, labels_ = read_hdf5(storedFilePath)\n",
    "\n",
    "    # print(f'[INFO] Read {len(images_)} images and {len(labels_)} labels')\n",
    "\n",
    "    # Checking if the read_image function works by comparing stored images and labels with the images and labels \n",
    "    # read from the file \n",
    "    assert(images.all()==images_.all())\n",
    "    assert(labels.all()==labels_.all())\n",
    "\n",
    "    # print(f'all good')\n",
    "\n",
    "# print(f'shape of images: {np.shape(images)}, type: {type(images)}')\n",
    "# print(f'shape of labels: {np.shape(labels)}, type: {type(labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:\\OneDrive\\Documents\\rPPG-Projects\\Datasets-Preprocessed\\hdf5\\UBFC2\\DATASET_2'\n",
    "\n",
    "# Store the paths of each subject into a list\n",
    "sub_paths = [os.path.join(data_dir, sub) for sub in os.listdir(data_dir)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  5  6  7  8  9 10 11 12 13 14 15 16 18 20 21 22 24 25 26 27 28\n",
      " 30 32 33 34 36 37 38 40 41]\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 23 24 25 26\n",
      " 28 29 31 35 36 37 39 40 41]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 11 12 13 15 16 17 19 21 22 23 25 27 28 29\n",
      " 30 31 32 33 34 35 37 38 39 40]\n",
      "[ 0  2  3  4  5  8  9 10 11 12 14 15 17 18 19 20 21 22 23 24 26 27 29 30\n",
      " 31 32 33 34 35 36 37 38 39 41]\n",
      "[ 1  2  3  4  6  7 10 13 14 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n",
      " 31 32 33 34 35 36 38 39 40 41]\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "data = np.array(sub_paths)\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "\t# print(f'train: {data[train]}, test: {data[test]}')\n",
    "\t# print(train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "721881c4ebbfc4a48f73e756600484e3f3653201792f48d5e9f9bb135cb791ab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
